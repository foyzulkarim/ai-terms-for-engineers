# Episode 03: LLMs আসলে কিভাবে কাজ করে? | Transformer to GPT-4 Journey

**Watch:** [YouTube](https://youtu.be/3YQ_D20MjBs)

---

## Transcript

### From Transformers to Large Language Models
Large Language Models-এর উদ্ভব হলো যখন রিসার্চাররা ট্রান্সফরমার আর্কিটেকচারকে অবিশ্বাস্য সাইজে স্কেল করা শুরু করলেন। মূল ইনসাইটটা ছিল—মডেলের সাইজ, ট্রেনিং ডেটা, আর কম্পিউট পাওয়ার বাড়ালে এমন সব নতুন সক্ষমতা (emergent capabilities) তৈরি হয় যা ছোট মডেলগুলোতে ছিল না। ২০২০ সালে OpenAI যখন ১৭৫ বিলিয়ন প্যারামিটারের GPT-3 রিলিজ দিল, তখন দেখা গেল যে যথেষ্ট বড় মডেলগুলো এমন সব টাস্ক পারফর্ম করতে পারছে যার জন্য তাদের আলাদাভাবে ট্রেইন করা হয়নি—শুধুমাত্র ভাষার প্যাটার্ন বুঝেই তারা এটা করছে।
LLM-গুলো unsupervised learning-এর মাধ্যমে ইন্টারনেট, বই, আর্টিকেল ইত্যাদি থেকে স্ক্র্যাপ করা বিশাল টেক্সট করপাসের (corpora) ওপর ট্রেইন করা হয়। ট্রেনিংয়ের উদ্দেশ্যটা আপাতদৃষ্টিতে খুব সিম্পল: একটা সিকোয়েন্সের পরের শব্দটা প্রেডিক্ট করা। এই সিম্পল কাজটাই মডেলকে গ্রামার, ফ্যাক্টস, রিজনিং প্যাটার্ন এবং এমনকি কিছুটা কমন সেন্স শিখতে বাধ্য করে যাতে সে সঠিক প্রেডিকশন করতে পারে। এর মাধ্যমে মডেলের মধ্যে ল্যাঙ্গুয়েজ স্ট্রাকচার, ওয়ার্ল্ড নলেজ এবং রিজনিং ক্ষমতার একটা ইন্টারনাল রিপ্রেজেন্টেশন তৈরি হয়।
চিন্তা করুন, "The capital of France is ____" এই বাক্যের পরের শব্দটা প্রেডিক্ট করতে হলে মডেলকে কী কী বুঝতে হবে। তাকে জিওগ্রাফি জানতে হবে, বুঝতে হবে "capital" মানে রাজধানী শহর, "France" একটা দেশ, এবং প্যারিস যে ফ্রান্সের রাজধানী সেই ফ্যাক্টটা রিট্রিভ করতে হবে। কোটি কোটি উদাহরণের ওপর ভিত্তি করে নেক্সট ওয়ার্ড প্রেডিক্ট করতে গিয়েই এই পুরো নলেজটা ন্যাচারালি তৈরি হয়।
GPT-4, Claude, এবং Gemini-র মতো মডার্ন LLM-গুলোর বিলিয়ন বা ট্রিলিয়ন প্যারামিটার থাকে। এগুলো ল্যাঙ্গুয়েজ ট্রান্সলেশন, প্রশ্নের উত্তর দেওয়া, কোড লেখা, ক্রিয়েটিভ রাইটিং এবং কমপ্লেক্স রিজনিংয়ের মতো অসাধারণ সব কাজ করতে পারে। এরা ইনস্ট্রাকশন ফলো করতে পারে, মাল্টি-টার্ন কনভারসেশন চালাতে পারে এবং প্রম্পটিংয়ের মাধ্যমেই বিভিন্ন টাস্কে অ্যাডাপ্ট করতে পারে—এর জন্য আলাদা কোনো টাস্ক-স্পেসিফিক ট্রেনিং লাগে না।
একাডেমিক রিসার্চ থেকে প্র্যাকটিক্যাল অ্যাপ্লিকেশনে এই ট্রানজিশনটা অবিশ্বাস্য দ্রুত ঘটেছে। যা ছিল নিছক রিসার্চ প্রজেক্ট, তা এখন কোটি কোটি মানুষের নিত্যদিনের প্রোডাক্ট। এই দ্রুত অ্যাডপশনের কারণ হলো মডেলগুলোর ভার্সেটালিটি—যে মডেল ইমেইল লিখতে পারে, সেই মডেলই কোড ডিবাগ করতে পারে, ডকুমেন্ট অ্যানালাইজ করতে পারে বা জটিল কনসেপ্ট বোঝাতে পারে। এই জেনারালাইজড ক্ষমতাই LLM-কে ন্যারো টুলের বদলে একটা পাওয়ারফুল প্ল্যাটফর্ম টেকনোলজিতে পরিণত করেছে।