# Episode 02: মডার্ন AI আসলে কিভাবে কাজ করে? | Deep Dive into "Attention Is All You Need"

**Watch:** [YouTube](https://youtu.be/7GV5X6gzz24)

---

## Transcript

### The Transformer Revolution (2017)
২০১৭ সালে Vaswani et al. এর "Attention is All You Need" পেপারটি পাবলিশ হওয়ার ঘটনাটা AI রিসার্চে একটা যুগান্তকারী মুহূর্ত ছিল। এই পেপারেই প্রথম Transformer আর্কিটেকচার ইন্ট্রোডিউস করা হয়, যা টেক্সটের মতো সিকোয়েন্সিয়াল ডেটা প্রসেসিংয়ের পুরো ধারণাটাই বদলে দেয়। ট্রান্সফরমারের আগে, ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিংয়ে recurrent neural networks (RNNs) এবং Long Short-Term Memory (LSTM) নেটওয়ার্কের দাপট ছিল, কিন্তু এগুলোর বেশ কিছু বড় সীমাবদ্ধতা ছিল।
RNN-গুলো টেক্সট প্রসেস করত সিকোয়েন্সিয়ালি, মানে একবারে একটা শব্দ। এর ফলে এরা বেশ স্লো ছিল এবং বাক্যের অনেক আগের অংশের সাথে পরের অংশের সম্পর্ক (long-range dependencies) ঠিকঠাক ধরতে পারত না। ধরুন একটা বাক্যের শুরুতে একটা গুরুত্বপূর্ণ শব্দ আছে, যার তাৎপর্য বাক্যের শেষে গিয়ে বোঝা যায়। RNN-এর ক্ষেত্রে শেষের শব্দে পৌঁছাতে পৌঁছাতে শুরুর শব্দের ইনফরমেশন ফিকে হয়ে যেত।
ট্রান্সফরমার আর্কিটেকচার নিয়ে এল self-attention কনসেপ্ট। এটা মডেলকে বাক্যের প্রতিটি শব্দের গুরুত্ব অন্য শব্দের সাপেক্ষে বিচার করার ক্ষমতা দিল, তাদের পজিশন যাই হোক না কেন। এর ফলে মডেল কনটেক্সট অনেক ভালো বুঝতে পারে এবং টেক্সটকে সিকোয়েন্সিয়ালি প্রসেস না করে প্যারালালি (parallelly) প্রসেস করতে পারে। সহজ কথায়, এটা অনেকটা পুরো বাক্যটা একবারে দেখে কোন শব্দগুলো একে অপরের সাথে কানেক্টেড সেটা বোঝার মতো, একটার পর একটা শব্দ পড়ে আগেরটা মনে রাখার চেষ্টা করার মতো না।
ট্রান্সফরমার আর্কিটেকচারে মূলত দুটি অংশ থাকে: encoders (যা ইনপুট টেক্সট প্রসেস করে) এবং decoders (যা আউটপুট টেক্সট জেনারেট করে)। Self-attention মেকানিজমের কারণে প্রতিটি শব্দ সিকোয়েন্সের অন্য সব শব্দের দিকে "মনযোগ" (attend) দিতে পারে, যা তৈরি করে রিচ কনটেক্সচুয়াল রিপ্রেজেন্টেশন। এই আর্কিটেকচার হাইলি স্কেলেবল প্রমাণিত হলো এবং GPT (Generative Pre-trained Transformer), BERT, এবং তাদের পরবর্তী সব মডার্ন LLM-এর ভিত্তি হয়ে উঠল।
এই ব্রেকথ্রু-র ফলেই মডেলগুলোকে ম্যাসিভ ডেটাসেটের ওপর ট্রেইন করা এবং বিলিয়ন বিলিয়ন প্যারামিটারে স্কেল করা সম্ভব হলো, যা আগে অসম্ভব ছিল। ট্রান্সফরমারের এফিসিয়েন্সি এবং ইফেক্টিভনেস একে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এবং তার বাইরের ডোমেইনেও ডমিন্যান্ট আর্কিটেকচারে পরিণত করল। ট্রান্সফরমার বুঝলে ইঞ্জিনিয়াররা বুঝতে পারবেন কেন মডার্ন LLM-গুলো এত ক্যাপবল এবং কেন এগুলোর জন্য এত হিউজ কম্পিউটেশনাল রিসোর্স লাগে। RNN-এর তুলনায় ট্রান্সফরমার ট্রেইন করা অনেক ফাস্টার, যা আজকের বিশাল সাইজের মডেলে পৌঁছানোর জন্য অত্যন্ত জরুরি ছিল।