<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Terms for Software Engineers</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reset.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/theme/black.css">
    <style>
        :root {
            --r-background-color: #0f1419;
            --r-main-font: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            --r-heading-font: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            --r-main-color: #e6e6e6;
            --r-heading-color: #ffffff;
            --r-link-color: #42b883;
            --r-link-color-hover: #35a372;
            --accent-color: #42b883;
            --secondary-color: #64b5f6;
        }

        .reveal {
            font-size: 32px;
        }

        .reveal h1 {
            font-size: 2.5em;
            font-weight: 700;
            text-shadow: 0 0 20px rgba(66, 184, 131, 0.3);
            margin-bottom: 0.5em;
        }

        .reveal h2 {
            font-size: 2em;
            font-weight: 600;
            color: var(--accent-color);
            margin-bottom: 0.8em;
            text-shadow: 0 0 15px rgba(66, 184, 131, 0.2);
        }

        .reveal h3 {
            font-size: 1.5em;
            font-weight: 600;
            color: var(--secondary-color);
            margin-bottom: 0.5em;
        }

        .reveal p, .reveal li {
            line-height: 1.6;
            margin-bottom: 0.8em;
        }

        .reveal ul {
            list-style: none;
            padding-left: 0;
        }

        .reveal li {
            position: relative;
            padding-left: 2em;
            margin-bottom: 0.8em;
        }

        .reveal li:before {
            content: "▸";
            position: absolute;
            left: 0;
            color: var(--accent-color);
            font-size: 1.2em;
            font-weight: bold;
        }

        .reveal .slide-number {
            background-color: rgba(66, 184, 131, 0.2);
            color: var(--accent-color);
            padding: 5px 10px;
            border-radius: 5px;
        }

        .reveal section {
            text-align: left;
            padding: 40px;
        }

        .reveal .title-slide {
            text-align: center;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            height: 100%;
        }

        .reveal .title-slide h1 {
            font-size: 3.5em;
            background: linear-gradient(135deg, #42b883 0%, #64b5f6 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
            margin-bottom: 0.3em;
        }

        .reveal .subtitle {
            font-size: 1.3em;
            color: #a0a0a0;
            font-weight: 300;
        }

        .section-divider {
            text-align: center !important;
            background: linear-gradient(135deg, rgba(66, 184, 131, 0.1) 0%, rgba(100, 181, 246, 0.1) 100%);
        }

        .section-divider h2 {
            font-size: 2.5em;
            margin: 0;
        }

        @media (max-width: 768px) {
            .reveal {
                font-size: 24px;
            }
        }

        /* GitHub Footer */
        .github-corner {
            position: fixed;
            top: 0;
            right: 0;
            z-index: 1000;
        }

        .github-corner svg {
            fill: var(--accent-color);
            color: var(--r-background-color);
            width: 80px;
            height: 80px;
        }

        .github-corner:hover svg {
            fill: var(--secondary-color);
        }

        .github-footer {
            position: fixed;
            bottom: 10px;
            left: 10px;
            z-index: 1000;
            font-size: 14px;
            background: rgba(15, 20, 25, 0.8);
            padding: 8px 15px;
            border-radius: 5px;
            backdrop-filter: blur(10px);
        }

        .github-footer a {
            color: var(--accent-color);
            text-decoration: none;
            display: flex;
            align-items: center;
            gap: 8px;
            transition: color 0.3s ease;
        }

        .github-footer a:hover {
            color: var(--secondary-color);
        }

        .github-footer svg {
            width: 20px;
            height: 20px;
            fill: currentColor;
        }

        @media (max-width: 768px) {
            .github-corner svg {
                width: 60px;
                height: 60px;
            }
            
            .github-footer {
                font-size: 12px;
                padding: 6px 10px;
            }

            .github-footer svg {
                width: 16px;
                height: 16px;
            }
        }
    </style>
</head>
<body>
    <!-- GitHub Corner Ribbon -->
    <a href="https://github.com/foyzulkarim/ai-terms-for-engineers" class="github-corner" aria-label="View source on GitHub" target="_blank">
        <svg viewBox="0 0 250 250" aria-hidden="true">
            <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
            <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
            <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
        </svg>
    </a>

    <!-- GitHub Footer Link -->
    <div class="github-footer">
        <a href="https://github.com/foyzulkarim/ai-terms-for-engineers" target="_blank" rel="noopener">
            <svg viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg">
                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"/>
            </svg>
            <span>Star on GitHub</span>
        </a>
    </div>

    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section class="title-slide">
                <h1>AI Terms for<br>Software Engineers</h1>
                <p class="subtitle">Essential Concepts for Working with<br>Large Language Models</p>
            </section>

            <!-- ============================================ -->
            <!-- PART 1: FOUNDATION - History & Architecture -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 1: Foundation</h2>
                <p class="subtitle">History & Architecture</p>
            </section>

            <!-- Machine Learning -->
            <section>
                <h2>Machine Learning: Teaching Computers to Learn from Data</h2>
                <ul>
                    <li class="fragment">Traditional programming: explicit instructions for every scenario</li>
                    <li class="fragment">Machine learning: algorithms learn patterns from training data automatically</li>
                    <li class="fragment">Neural networks: layers of interconnected nodes that mimic brain structure</li>
                    <li class="fragment">Training process: adjust weights and biases to minimize prediction errors</li>
                </ul>
            </section>

            <!-- Attention Is All You Need -->
            <section>
                <h2>"Attention Is All You Need"<br>Changed Everything (2017)</h2>
                <ul>
                    <li class="fragment">Breakthrough paper introduced Transformer architecture</li>
                    <li class="fragment">Key innovation: attention mechanism replaces recurrence and convolutions</li>
                    <li class="fragment">Encoder-decoder structure processes sequences in parallel</li>
                    <li class="fragment">Foundation for modern LLMs: GPT, BERT, and beyond</li>
                    <li class="fragment">Enabled massive scaling of language models</li>
                </ul>
            </section>

            <!-- From Transformers to LLMs -->
            <section>
                <h2>From Transformers to<br>Large Language Models</h2>
                <ul>
                    <li class="fragment">LLMs: deep learning models pre-trained on massive text datasets</li>
                    <li class="fragment">Self-supervised learning on billions of tokens</li>
                    <li class="fragment">Capable of understanding and generating human-like text</li>
                    <li class="fragment">Foundation models can be adapted for multiple tasks</li>
                    <li class="fragment">Examples: GPT-4, Claude, Llama, Gemini</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 2: CORE CONCEPTS - The Building Blocks -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 2: Core Concepts</h2>
                <p class="subtitle">The Building Blocks</p>
            </section>

            <!-- Parameters -->
            <section>
                <h2>Parameters: The Model's Knowledge</h2>
                <ul>
                    <li class="fragment">Parameters = weights and biases in neural network layers</li>
                    <li class="fragment">They encode all learned patterns from training data</li>
                    <li class="fragment">Model size measured by parameter count (e.g., 7B, 70B, 175B)</li>
                    <li class="fragment">More parameters = greater capacity for complex patterns</li>
                    <li class="fragment">GPT-3: 175 billion parameters</li>
                </ul>
            </section>

            <!-- Tokens -->
            <section>
                <h2>Tokens: The Language<br>of LLMs</h2>
                <ul>
                    <li class="fragment">Tokens: words, subwords, or character sequences</li>
                    <li class="fragment">Tokenization breaks text into discrete units for processing</li>
                    <li class="fragment">LLMs predict the next token in a sequence</li>
                    <li class="fragment">Training data measured in tokens (trillions of tokens)</li>
                    <li class="fragment">Different tokenizers: BPE, WordPiece, SentencePiece</li>
                </ul>
            </section>

            <!-- Context Window -->
            <section>
                <h2>Context Window:<br>The Model's Working Memory</h2>
                <ul>
                    <li class="fragment">Context window: maximum tokens the model can process at once</li>
                    <li class="fragment">Common sizes: 4K, 8K, 32K, 128K, 1M+ tokens (varies by model)</li>
                    <li class="fragment">Includes both input prompt and generated output</li>
                    <li class="fragment">Exceeding the limit requires chunking or summarization strategies</li>
                    <li class="fragment">Affects application design and cost (longer context = higher cost)</li>
                </ul>
            </section>

            <!-- GPU Memory -->
            <section>
                <h2>Running LLMs Requires Significant GPU Memory (VRAM)</h2>
                <ul>
                    <li class="fragment">VRAM is the primary bottleneck for LLM deployment</li>
                    <li class="fragment">Rule of thumb: model size in GB ≈ VRAM needed for inference</li>
                    <li class="fragment">Training: ~16GB VRAM per billion parameters (full precision)</li>
                    <li class="fragment">Consumer GPUs: 8-24GB (suitable for smaller models)</li>
                    <li class="fragment">Enterprise GPUs: 40-80GB+ (NVIDIA H100, A100)</li>
                    <li class="fragment">Quantization dramatically reduces requirements</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 3: DATA & RETRIEVAL -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 3: Data & Retrieval</h2>
                <p class="subtitle">Connecting Knowledge</p>
            </section>

            <!-- Embeddings -->
            <section>
                <h2>Embeddings: Converting Text to Numerical Vectors</h2>
                <ul>
                    <li class="fragment">Numerical representations that capture semantic meaning</li>
                    <li class="fragment">Similar concepts have similar vector representations</li>
                    <li class="fragment">Enable semantic search and similarity matching</li>
                    <li class="fragment">Foundation for building RAG systems</li>
                    <li class="fragment">Dimensions typically range from 384 to 1536+</li>
                </ul>
            </section>

            <!-- Vector Databases -->
            <section>
                <h2>Vector Databases: Storing and Searching Embeddings at Scale</h2>
                <ul>
                    <li class="fragment">Specialized databases optimized for vector similarity search</li>
                    <li class="fragment">Popular options: Pinecone, Weaviate, Qdrant, Chroma, FAISS</li>
                    <li class="fragment">Support for CRUD operations, metadata filtering, hybrid search</li>
                    <li class="fragment">Essential infrastructure for RAG and semantic search applications</li>
                    <li class="fragment">Choose based on scale, performance needs, and deployment model</li>
                </ul>
            </section>

            <!-- RAG -->
            <section>
                <h2>RAG Combines Retrieval with Generation for Accuracy</h2>
                <ul>
                    <li class="fragment">Retrieval-Augmented Generation (RAG): fetch relevant data before generating</li>
                    <li class="fragment">Semantic search: understands intent, not just keywords</li>
                    <li class="fragment">Uses vector embeddings to find conceptually similar content</li>
                    <li class="fragment">Reduces hallucinations with grounded information</li>
                    <li class="fragment">Enables up-to-date responses beyond training data</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 4: MODEL BEHAVIOR & TRAINING -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 4: Model Behavior</h2>
                <p class="subtitle">Training & Optimization</p>
            </section>

            <!-- Inference Parameters -->
            <section>
                <h2>Inference Parameters: Controlling Output Quality and Creativity</h2>
                <ul>
                    <li class="fragment">Temperature: controls randomness (0 = deterministic, 1+ = creative)</li>
                    <li class="fragment">Top-p (nucleus sampling): considers tokens with cumulative probability p</li>
                    <li class="fragment">Top-k: limits selection to k most likely tokens</li>
                    <li class="fragment">Higher temperature = more creative but less predictable</li>
                    <li class="fragment">Adjust based on use case: factual (low temp) vs creative (high temp)</li>
                </ul>
            </section>

            <!-- Fine-Tuning -->
            <section>
                <h2>Fine-Tuning: Specializing Pre-trained Models</h2>
                <ul>
                    <li class="fragment">Start with pre-trained foundation model</li>
                    <li class="fragment">Further train on domain-specific dataset</li>
                    <li class="fragment">Requires far less data and compute than training from scratch</li>
                    <li class="fragment">Hundreds to thousands of examples often sufficient</li>
                    <li class="fragment">Customizes model behavior for your application</li>
                </ul>
            </section>

            <!-- RLHF -->
            <section>
                <h2>Reinforcement Learning from<br>Human Feedback (RLHF)</h2>
                <ul>
                    <li class="fragment">Humans provide feedback on model outputs</li>
                    <li class="fragment">Reward model learns to score responses</li>
                    <li class="fragment">LLM optimized using reinforcement learning</li>
                    <li class="fragment">Critical for helpful, harmless, and honest AI</li>
                </ul>
            </section>

            <!-- Quantization -->
            <section>
                <h2>Quantization: Shrinking Models Efficiently</h2>
                <ul>
                    <li class="fragment">Converts 32-bit/16-bit weights to 8-bit or 4-bit representations</li>
                    <li class="fragment">8-bit quantization: 50% memory savings, &lt;1% accuracy loss</li>
                    <li class="fragment">4-bit quantization: 75% memory savings, 2-5% accuracy drop</li>
                    <li class="fragment">Enables running larger models on consumer hardware</li>
                    <li class="fragment">Trade-off: memory efficiency vs. model quality</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 5: WORKING WITH LLMs - Application Layer -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 5: Working with LLMs</h2>
                <p class="subtitle">Application Layer</p>
            </section>

            <!-- Prompt Engineering -->
            <section>
                <h2>Prompt Engineering: Designing Effective Instructions</h2>
                <ul>
                    <li class="fragment">Crafting clear instructions to guide LLM outputs</li>
                    <li class="fragment">Zero-shot: no examples, model uses general knowledge</li>
                    <li class="fragment">Few-shot: provide 2-5 examples to guide response format</li>
                    <li class="fragment">Chain-of-thought: ask model to "think step-by-step"</li>
                    <li class="fragment">Critical skill for building LLM-powered applications</li>
                </ul>
            </section>

            <!-- Structured Output -->
            <section>
                <h2>Structured Output: Ensuring Reliable JSON and Data Formats</h2>
                <ul>
                    <li class="fragment">JSON mode: forces LLM to output valid JSON</li>
                    <li class="fragment">Constrained generation: schema validation, type enforcement</li>
                    <li class="fragment">Critical for parsing LLM responses programmatically</li>
                    <li class="fragment">Reduces parsing errors and improves reliability</li>
                    <li class="fragment">Supported natively by major API providers</li>
                </ul>
            </section>

            <!-- Function Calling -->
            <section>
                <h2>Function Calling: Extending LLMs<br>with External Tools</h2>
                <ul>
                    <li class="fragment">LLMs can call external functions/APIs based on user intent</li>
                    <li class="fragment">Define function schemas, LLM decides when and how to call them</li>
                    <li class="fragment">Enables actions: database queries, API calls, calculations</li>
                    <li class="fragment">Supported by OpenAI, Anthropic (Claude), Google (Gemini)</li>
                    <li class="fragment">Essential for building agentic and interactive applications</li>
                </ul>
            </section>

            <!-- Agent Workflows -->
            <section>
                <h2>Agent Workflows: Autonomous AI Systems</h2>
                <ul>
                    <li class="fragment">Agentic AI: autonomous systems that reason, plan, and act</li>
                    <li class="fragment">Multi-step workflows with tool use and decision-making</li>
                    <li class="fragment">Can access external APIs, databases, and services</li>
                    <li class="fragment">Iterate toward goals with minimal human intervention</li>
                    <li class="fragment">Evolution from chatbots to autonomous assistants</li>
                </ul>
            </section>

            <!-- Multi-modal Models -->
            <section>
                <h2>Multi-modal Models:<br>Beyond Text to Vision and Audio</h2>
                <ul>
                    <li class="fragment">Process multiple modalities: text, images, audio, video</li>
                    <li class="fragment">Vision-language models: GPT-4V, Claude 3, Gemini Pro Vision</li>
                    <li class="fragment">Use cases: image analysis, OCR, visual Q&A, document understanding</li>
                    <li class="fragment">Audio models: Whisper (speech-to-text), voice synthesis</li>
                    <li class="fragment">Expanding AI capabilities beyond pure text processing</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 6: PRODUCTION & INTEGRATION -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 6: Production & Integration</h2>
                <p class="subtitle">Deploying to Production</p>
            </section>

            <!-- LLM APIs -->
            <section>
                <h2>LLM APIs: Integrating AI<br>into Your Applications</h2>
                <ul>
                    <li class="fragment">Major providers: OpenAI, Anthropic, Google AI, AWS Bedrock</li>
                    <li class="fragment">RESTful APIs with simple request/response structure</li>
                    <li class="fragment">Pricing: typically per-token (input + output tokens)</li>
                    <li class="fragment">Rate limiting and quota management required</li>
                    <li class="fragment">Authentication via API keys, SDKs for major languages</li>
                </ul>
            </section>

            <!-- Choosing the Right Model -->
            <section>
                <h2>Choosing the Right Model:<br>Balancing Cost, Speed, and Quality</h2>
                <ul>
                    <li class="fragment">Hosted APIs vs self-hosted: convenience vs control and cost</li>
                    <li class="fragment">Model size trade-offs: larger = better quality, slower, more expensive</li>
                    <li class="fragment">Latency considerations: smaller models for real-time applications</li>
                    <li class="fragment">Specialized models: code (Codex), chat (ChatGPT), embeddings</li>
                    <li class="fragment">Start with hosted APIs, self-host only when necessary</li>
                </ul>
            </section>

            <!-- LLM Evaluation -->
            <section>
                <h2>LLM Evaluation:<br>Measuring Model Performance</h2>
                <ul>
                    <li class="fragment">Benchmarks: MMLU (knowledge), HumanEval (coding), HellaSwag (reasoning)</li>
                    <li class="fragment">Leaderboards track model performance across tasks</li>
                    <li class="fragment">Custom evaluation: define metrics for your specific use case</li>
                    <li class="fragment">A/B testing for comparing prompts or models</li>
                    <li class="fragment">Human evaluation often necessary for quality assessment</li>
                </ul>
            </section>

            <!-- ============================================ -->
            <!-- PART 7: PRODUCTION CONCERNS -->
            <!-- ============================================ -->
            
            <section class="section-divider">
                <h2>Part 7: Production Concerns</h2>
                <p class="subtitle">Reliability, Cost & Security</p>
            </section>

            <!-- Hallucinations -->
            <section>
                <h2>Hallucinations: When LLMs Generate Plausible but False Information</h2>
                <ul>
                    <li class="fragment">LLMs can confidently generate incorrect or fabricated information</li>
                    <li class="fragment">Caused by training data gaps, pattern matching without understanding</li>
                    <li class="fragment">Mitigation: RAG, fact-checking, citations, temperature reduction</li>
                    <li class="fragment">Critical for high-stakes applications (medical, legal, financial)</li>
                    <li class="fragment">Always verify LLM outputs in production systems</li>
                </ul>
            </section>

            <!-- Cost Optimization -->
            <section>
                <h2>Cost Optimization: Managing Token Usage and Expenses</h2>
                <ul>
                    <li class="fragment">Token counting: both input and output tokens are billed</li>
                    <li class="fragment">Caching strategies: reuse embeddings, cache common prompts</li>
                    <li class="fragment">Model selection: use smaller models when quality difference is minimal</li>
                    <li class="fragment">Prompt optimization: be concise, avoid unnecessary context</li>
                    <li class="fragment">Monitor usage and set budget alerts</li>
                </ul>
            </section>

            <!-- Security -->
            <section>
                <h2>Security Considerations:<br>Protecting Against LLM Vulnerabilities</h2>
                <ul>
                    <li class="fragment">Prompt injection: malicious inputs that override instructions</li>
                    <li class="fragment">PII handling: avoid sending sensitive data to external APIs</li>
                    <li class="fragment">Content filtering: detect and block harmful outputs</li>
                    <li class="fragment">Input validation: sanitize user inputs before sending to LLM</li>
                    <li class="fragment">Implement guardrails and monitoring for production systems</li>
                </ul>
            </section>

        </div>
    </div>

    <script src="https://cdn.jsdelivr.net/npm/reveal.js@4.3.1/dist/reveal.js"></script>
    <script>
        window.addEventListener('load', function() {
            console.log('Page loaded, initializing Reveal.js...');
            console.log('Total sections found:', document.querySelectorAll('.slides section').length);
            
            Reveal.initialize({
                hash: true,
                slideNumber: true,
                transition: 'slide',
                backgroundTransition: 'fade',
                controls: true,
                progress: true,
                center: false,
                width: 1280,
                height: 720,
                margin: 0.04,
                keyboard: true,
                overview: true,
                touch: true
            }).then(() => {
                console.log('Reveal.js initialized successfully');
                console.log('Total slides:', Reveal.getTotalSlides());
                console.log('Current slide:', Reveal.getIndices());
            }).catch(err => {
                console.error('Error initializing Reveal.js:', err);
            });

            Reveal.on('slidechanged', event => {
                console.log('Changed to slide:', event.indexh, 'Total:', Reveal.getTotalSlides());
            });
        });
    </script>
</body>
</html>
